
********************************************
###F:\work2\10_python\python\CopareWords.py###
********************************************



import re # 正则表达式库
import jieba # 结巴分词

'''
比较两个文档中的单词
筛选出各自不重复的单词和共通的单词



'''




def analyse(file):
    # 读取文件
    fn = open(file,'r',-1,'utf8') # 打开文件
    string_data = fn.read() # 读出整个文件
    fn.close() # 关闭文件


    pattern = re.compile(u'\t|\n|\.|-|:|;|\)|\(|\?|"|\[|\]|\d+') # 定义正则表达式匹配模式
    string_data = re.sub(pattern, ' ', string_data) # 将符合模式的字符去除
    #print("打印过滤")
    #print(string_data)

    punct = set(u''':!),.:;?]}¢'"、。〉》」』】〕〗〞︰︱︳﹐､﹒
    ﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠
    々‖•·ˇˉ―--′’”([{£¥'"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻
    ︽︿﹁﹃﹙﹛﹝（｛“‘-—_… ''')

    # 文本分词
    seg_list_exact = jieba.cut(string_data, cut_all = False) # 精确模式分词


    object_list = []
    object_list_real = []
    n=0;
    for word in seg_list_exact: # 循环读出每个分词
        wordtmp = word.lower()
        if wordtmp not in punct:
            n+=1
            if wordtmp not in object_list and len(wordtmp) > 1: # 如果不在去除词库中
                object_list.append(wordtmp) # 分词追加到列表
                object_list_real.append(word)

    print("%s %s" % (file,  n))
    #print(object_list)
    return  object_list

object_list1 = analyse("word1")
object_list2 = analyse("word2")

common=[]
wordOnlyIn2=[]
wordOnlyIn1=[]
for word in object_list1:
    if word in object_list2:
        common.append(word)
    else:
        wordOnlyIn1.append(word)


for word in object_list2:
    if word not in common:
        wordOnlyIn2.append(word)

print("共通：")
print(len(common))
print("词本1独占：")
print(len(wordOnlyIn1))
print("词本2独占：")
print(len(wordOnlyIn2))
print(common)
print(wordOnlyIn1)
print(wordOnlyIn2)

********************************************
###F:\work2\10_python\python\CountWords.py###
********************************************

# 导入扩展库
import re # 正则表达式库
import collections # 词频统计库
import numpy as np # numpy数据处理库
import jieba # 结巴分词
import wordcloud # 词云展示库
from PIL import Image # 图像处理库
import matplotlib.pyplot as plt # 图像展示库



'''
统计文档中的单词数目，并列出使用频率
'''

#改进 1 特殊字符 2 输出排序 3 字符过滤 4 百分比
punct = set(u''':!),.:;?]}¢'"、。〉》」』】〕〗〞︰︱︳﹐､﹒
﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠
々‖•·ˇˉ―--′’”([{£¥'"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻
︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')

# 读取文件
fn = open('words','r',-1,'utf8') # 打开文件
string_data = fn.read() # 读出整个文件
fn.close() # 关闭文件

# 文本预处理
pattern = re.compile(u'\t|\n|\.|-|:|;|\)|\(|\?|"|\[|\]') # 定义正则表达式匹配模式
string_data = re.sub(pattern, '', string_data) # 将符合模式的字符去除

# 文本分词
seg_list_exact = jieba.cut(string_data, cut_all = False) # 精确模式分词
object_list = []
remove_words = [u' ',u' ',u','] # 自定义去除词库

for word in seg_list_exact: # 循环读出每个分词
    if word not in remove_words and word not in object_list: # 如果不在去除词库中
        object_list.append(word) # 分词追加到列表

# 词频统计
word_counts = collections.Counter(object_list) # 对分词做词频统计
word_counts_top10 = word_counts.most_common(3000) # 获取前10最高频的词

#print (word_counts_top10) # 输出检查
n=0
countAll = len( object_list)
print(countAll)

print(len(word_counts))
for tmp in word_counts_top10:
    n+=1
   # print("%s	%s	%s	%s" %  (n,tmp[0],tmp[1],tmp[1]/countAll))


********************************************
###F:\work2\10_python\python\imageOCR.py###
********************************************


# -*- coding: utf-8 -*-
from PIL import Image
import pytesseract
#上面都是导包，只需要下面这一行就能实现图片文字识别
text=pytesseract.image_to_string(Image.open('aa.JPEG'),lang='chi_sim') #设置为中文文字的识别
#text=pytesseract.image_to_string(Image.open('tt.jpg'),lang='eng')   #设置为英文或阿拉伯字母的识别
print(text)
********************************************
###F:\work2\10_python\python\spider.py###
********************************************

#coding=utf-8
import urllib
import urllib.request
import re

def getHtml(url):
	user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
	values = {'name' : 'WHY',
	          'location' : 'SDU',
	          'language' : 'Python' }
	headers = { 'User-Agent' : user_agent }
	data = urllib.parse.urlencode(values)
	proxy_handler = urllib.request.ProxyHandler({'http': 'proxy.zte.com.cn'})
	#opener = urllib.request.build_opener(proxy_handler,data,headers)
	opener = urllib.request.build_opener(proxy_handler)
	r = opener.open(url)
	html = r.read()
	return html

html = getHtml("http://it.zte.com.cn/its/login/ssoLogin.action?rand=1498037115649")
def getImg(html):
    reg =  'src="(.*?\.js)"'
    imgre = re.compile(reg)
    imglist = re.findall(imgre,html)
    return imglist
print ("html：  %s " % html )

print ("IMGAGE ： %s " % getImg(html.decode("utf-8")))
imglist = getImg(html.decode("utf-8"))
x=0
for imgurl in imglist:
	if imgurl.find("http:") == -1 :
		imgurl = 'http://it.zte.com.cn' + imgurl

	print("IMAGEURL：%s " % imgurl)

	urllib.request.urlretrieve(imgurl,'%s' % imgurl[imgurl.rfind("/")+1:])
	x+=1

********************************************
###F:\work2\10_python\python\spiderFriend.py###
********************************************

# coding=utf-8
import urllib
import urllib.request
import re
import http.cookiejar
from html.parser import HTMLParser
from bs4 import BeautifulSoup
import pymysql.cursors
import time
import datetime
import os

'''
spide to find friend and sava info
'''

#https://www.jianshu.com/p/30b4fa6793f6


def getHtml(url):
    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
    values = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
              'location': 'SDU',
              'language': 'Python'}
    cookieA='''TECH_LOGIN_NAME_COOKIE_KEY=0216000799; TECH_LOGIN_REMEBER_COOKIE_KEY=false; TECH_LOGIN_ERROR_COOKIE_KEY=""; JSESSIONID=C3473CFBFD1243642C5FF7B270B8A14A; TECH_LOGIN_INDEX_ID_COOKIE_KEY=0216000799; TECH_LOGIN_ID_COOKIE_KEY=0216000799; TECH_LOGIN_EMPIDUI_COOKIE_KEY=0216000799; TECH_LOGIN_CHNAME_COOKIE_KEY=5ZSQ6ZOt; TECH_LOGIN_ENNAME_COOKIE_KEY="dGFuZy9taW5nLw=="; ZTEDPGSSOUser=0216000799; ZTEDPGSSOLanguage=zh_CN; ZTEDPGSSOVersionType=1; udsVersion=V3.6.0build21; clientSerialNo=DC-4A-3E-59-96-A7; weibojs_100010=access_token%3D2.00QBmcdCuKRwQB674d274608FZkwEC%26refresh_token%3D%26expires_in%3D451780%26uid%3D0216000799%26status%3D1%26indexId%3D0216000799%26loginId%3D0216000799%26userType%3D0; FOL_Language=zh-CN; TMS_SessionId=MkY4QjBFMjQwRTE4QzQyODU1MDQxMkNCQTBCNzJGRjg=; TMS_Language=zh-CN; TMS_SysParams=QjFlRjMxS0NPSjU2MzhUcmhjRkd2dTZZL3FJalBHRGpIcjYrZTE2NThKOHg2TnYvcU9HUjhEMDByK2hvMmgzdnBuVU9HTmZSLy9BN1NBZVBQRVFOeXRoSElWNXlPbnVMSHBCNTg4TnIxa0JpZFVHZXN4cExiaGUxNTBEZU1QUGphNDZmbDJ2UXpZZFFDWmJsNjlGbDhOYnF5czRMcE5Kc3oybUFMN3ZnVmRKdHljRzZncTNQbEI2QjBiTlovczhDQ1cwbE55ejNtdFJlSVpwdVNqUGZFaktMUlZZTG1wbGR6QzEzbi9hWWwwUmFqQnNyM1BXNmx5V2NGK0FCY3llWQ==; times=17; FOL_SessionId=NTIxRjU5RkUwQUU3REEyRjlCQkQ0NDg0RDk0MEYxRDI=; FOL_SysParams=K1FqeHkvTC9OeXc5U1VNaDdtSnN5ZnV1Z2FzcUIvRDZVUktRVlhVaVp5anQ4eDNyL0M0dk9FTXdSMU9zRHpjUVBpdEJHbHRvMGxtMjVyV3hnaDVScndpR1J2YzdvdVNaY2FCRDBSUW44b3NRNFR3L21SUzYxaDJUR1VWR21nZWJUWjMyeWFHajhNRmRPZmN4b25qalVxaTNnNTh5dXRoUWJVUjh0N0c0Q2dmMFBxZTN2OXNMaFozYTVGbk4wSmhMQUtSNGpWRGFneWdPRnNnU1hXcHlmNjM0amhmSFN6RFVPQm5BemQ3NkxNWT0=; ZTEDPGSSOCookie=a3e5e03bc41a7ea37b06c5593121e42d; udsClientIp=10.42.193.171; SHARE_EmployeeNum=0216000799; SHARE_SessionId=3EA0732BB0566B1D40E6D22279ED1E32; SHARE_Language=zh; SHARE_SysParams=TzRWZ1FUQTVlQVlZa2tjN0VHUFFlQVZ2aitFQWZvRmN5eEZYcnBReURJVUpMM1U0TkZjQUEyTWsvQ0E2Y3Q2dGFXcnlTcmIxdW1mV1BJQkVQaFROZFpEMXdrdFBSRXlzbEMrUklFR2FrSTIrSWlvc2RHbnROekk0dUtJeExSYlNlY3ZPdjFLbHpveUMwOXl2ZTdsNXBGZzMraHErUWdWK1d2QzYxcUhXTzBnRFNxRE1SMTRvT2xJS3kxQ3BpRzlFUzBpYmRwQ014SkNCWVhQZmhUbXNCY0FQK0x2TVdEekFJTElHMk5zRW43YWZsby8wVkk4RWFueW9NTzdwNVRCNW9qcnF0Mlk2VGxsV0F1QnNKYUo2OTdzNFBjc09VMnUxSFpKY0FERXBFNHc9; SHARE_CustomParams=Y3VzdG9tUGFyYW1z; tech-username="DOYt+ODIK69EtfW6oxqrcA__"; tech-password="Rh2JLaMM3Lg_"; tech-rememberme="false"; tech-validation="b6c7b9e68afb1813eeaa5b0df70b823c"'''
    headers = {
        "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Encoding":"utf-8",
        "Accept-Language":"zh-cn,zh;q=0.8,en-us;q=0.5,en;q=0.3",
        "Connection":"keep-alive",
        "Host":"share.zte.com.cn",
        "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36",
        "Cookie":cookieA
    }
    data = urllib.parse.urlencode ( values )
    proxy_handler = urllib.request.ProxyHandler ( {'http': 'proxy.zte.com.cn'} )
 #   opener = urllib.request.build_opener(proxy_handler,data,headers)
 #  urllib.request.install_opener(opener)
    req = urllib.request.Request(url, None, headers)
    response = urllib.request.urlopen(req)
#    response3 = urllib.request.urlopen(url)
    html =response.read()
    print(response.code)
    print(html)
    return html

def get(urlList):
    for url in urlList:
        html=getHtml (url)
        parse(html)

def _attr(attrlist, attrname):
        for attr in attrlist:
            if attr == attrname:
                return attr
        return None

config = {
        'host': '127.0.0.1',
        'port': 3306,
        'user': 'root',
        'password': '',
        'db': 'log',
        'charset': 'utf8mb4',
        'cursorclass': pymysql.cursors.DictCursor,
    }

#
connection = pymysql.connect(**config)

def execte(sql):
        # 执行sql语句
        try:
            with connection.cursor() as cursor:
                # 执行sql语句，进行查询
                cursor.execute(sql)
        finally:
            x=1

#获取主要DIV
def parse(html):
    soup = BeautifulSoup(html,'html.parser')
    #print(soup.prettify())
    data = soup.findAll('div',{"class" : "feedCell"}  )
    #data = soup.findAll('p' )
    #print(data[0])
    if len(data) == 0:
        return
    for tmp in data:
        print("***************************************")
        parseDetail(tmp)

#细分数据
def parseDetail(data):
    mainDiv=data.find('div',{"class":"oriTxt"})
    #
    regex = re.compile("^topic_content\w+_short$")
    spanContent=mainDiv.findAll('span')
    info=''
    for span in spanContent:
        if not _attr(span.attrs,"id") is None and regex.match(span['id']):
            print("span2: %s" % span.get_text())

            info=span.get_text()

    #

    authorContent=mainDiv.find('p',{"class":"utitle"}).find('span',{"class":"un"}).find('a').string.strip()
    print("author %s " % authorContent)


    imagenContent=mainDiv.find('ul',{"class":"imgList"})
    imageString=''
    if imagenContent is not None:
        imagenContent=   imagenContent.findAll('img')

        for span in imagenContent:
            imgurl=span['src']
            if not imgurl.endswith("icon.png"):
                print("image: %s" % span['src'])
                imageName=authorContent+"_"+imgurl[imgurl.rfind ( "/" ) + 1:]
                imageName=imageName.replace('_s.','.')
                imageString=imageString +( "./image/"+imageName) + ","
                imgurl=imgurl.replace('_s.','.')
                try :
                    urllib.request.urlretrieve ( imgurl, './image/%s' %  imageName )
                except urllib.error.HTTPError:
                    print (11)
                finally:
                    x=1



    dateContent=mainDiv.find('span',{"class":"fromWhere"}).find('a').string.strip()

    dt=datetime.datetime.now().strftime("%m--%d## ")
    dt1=(datetime.datetime.now()+datetime.timedelta(-1)).strftime("%m--%d## ")
    dt2=(datetime.datetime.now()+datetime.timedelta(-2)).strftime("%m--%d## ")
    if dateContent.find("天") == -1 and dateContent.find("月") == -1 :
        dateContent=dt+dateContent
    else:
        dateContent=dateContent.replace("前天",dt2).replace("昨天",dt1)

    dateContent=dateContent.replace("--","月").replace("##","日")

    if dateContent.find("2018年") == -1 :
        dateContent="2019年"+dateContent

    dateContent=dateContent.replace("年","-").replace("月","-").replace("日","").replace("时",":").replace("分",":00")

    print("date %s " % dateContent)
    if dateContent.find("钟前") ==-1 and judgeNewDate(dateContent) == True:
        sql="insert into log.friend (author,info  ,image ,date ,tag ) VALUES ('%s','%s','%s','%s','')" % (authorContent,info,imageString,dateContent)
        print(sql)
        execte(sql)

def getNewDateStart():
            try:
                with connection.cursor() as cursor:
                    # 执行sql语句，进行查询
                    cursor.execute("select max(date) as date , min(date)  as dateEnd from friend")
                    result=cursor.fetchone()
                    print ("dateSqlStart %s" % result['date'])
                    return result['date']
            finally:
                x=1;
            return null;
def getNewDateEnd():
            try:
                with connection.cursor() as cursor:
                    # 执行sql语句，进行查询
                    cursor.execute("select max(date) as date , min(date)  as dateEnd from friend")
                    result=cursor.fetchone()
                    print ("dateSQLEND %s" %  result['date'])
                    return result['dateEnd']
            finally:
                x=1;
            return null;


dateStart=getNewDateStart();
dateEnd=getNewDateEnd();


def judgeNewDate(dateContent):
            # 执行sql语句
        print("******#####")
        print("dateStart %s" % dateStart)
        print("dateEnd %s" % dateEnd)

        dateContent2=dateContent.replace("年","-").replace("月","-").replace("日","").replace("时",":").replace("分","")
        print("dateContent2 %s" %  dateContent2)
        dateNew = datetime.datetime.strptime(dateContent2, '%Y-%m-%d %H:%M:%S')

        if dateNew >dateStart or dateNew < dateEnd:
            return  True

        return False;

def formatImg(Img):
    x=1
urlList=[]
for i in range(1,4):
    url = "http://share.zte.com.cn/tech/jsp/pageWeibo?id=d7403aa5b9e847e1a61bc5615d82d74f&currentPage="+str(i)+"&circlePurview=1&serachContext=&searchUserName="
    urlList.append(url)
get(urlList)
connection.commit()
connection.close();

os.system("python sql.py")
********************************************
###F:\work2\10_python\python\sql.py###
********************************************

import pymysql.cursors

import os

'''

generate html for friend  
'''


config = {
        'host': '127.0.0.1',
        'port': 3306,
        'user': 'root',
        'password': '',
        'db': 'log',
        'charset': 'utf8mb4',
        'cursorclass': pymysql.cursors.DictCursor,
    }
#
connection = pymysql.connect(**config)

head='''
<html>
<head>
    <script src="jquery-3.3.1.js"></script>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script>
        function aa() {

            var ids = document.getElementsByName("Fruit");
            var idstr = '0,';
            var sqls = '';
            var sql='update friend set tag=concat(tag,"%aa",",") where id = %id;'
            for(var i=0;i<ids.length;i++){
                sql='update friend set tag=concat(tag,"%aa",",") where id = %id;'

                if(document.getElementsByName('FruitValue')[i].value != '') {
                    sql = sql.replace("%aa"    ,document.getElementsByName('FruitValue')[i].value).replace("%id"    , ids [i].value);
                    idstr = idstr + "," + ids [i].value;
                    sqls= sqls+sql;
                }

            }
            idstr = 'update friend set tag=tag+"" where id in(' + idstr + ')'
            console.log(sqls);

            alert(sqls);
        }

        function chageTag(){

            var tag ="." + $("#switch").val();
            if(tag == ".") {

                $.each($("tr"), function(i){
                      this.style.display = 'block';
                });
                return
            }

            $.each($("tr"), function(i){
                  this.style.display = 'none';
            });

             $.each($(tag), function(i){
                  this.style.display = 'block';
            });
        }

        function toggleImage() {
             $.each($("img"), function(i){
                if( this.style.display == 'none') {
                  this.style='';
                 } else {
                  this.style.display = 'none';
                 }
            });

        }

    </script>
</head>
<body>
    数量：<label>%count</label>
    <form action="aa.do">
    <input type="button"   value="Submit" onclick="aa()">
    <input type="button"   value="Picture" onclick="toggleImage()">

    <select onchange="chageTag()" id="switch">
        <option value="">无</option>
        <option value="了解中">了解中</option>
        <option value="冷淡">冷淡</option>

    </select>
    <table>

'''

tail='''
    </table>
    <input type="button"   value="Submit" onclick="aa()">
    </form>
</body>
</html>
'''

tr='''
        <tr class="%class">
            <td style="width: 1000px">
            *********************************************************************************************************************<br>
            <br>

            <!-- -->
            <lable>%tag</lable>

            <br>
                %info
            </td>
        </tr>

        <tr class="%class">
            <td align="left">
                %image
            <br>
            标签：<input name="Fruit" type="text" hidden value="%id" />
            <input name="FruitValue" type="text" value="" />
            </td>

        </tr>
        <tr class="%class">
            <td align="left">
                发布人: %author
                日  期: %date
            </td>
        </tr>
'''

def execte(sql):
        # 执行sql语句
        try:
            os.remove("friend.html")
            f=open('friend.html', 'a+', encoding='utf8')
            with connection.cursor() as cursor:
                # 执行sql语句，进行查询
                cursor.execute("UPDATE 	  friend set tag=concat(tag, '异地,')    where tag not like '%异地%' and ( info like '%深圳%' or info like '%西安%'or info like '%武汉%'or info like '%上海%'or info like '%bbbbb%'or info like '%bbbbb%')")
                cursor.execute("delete from friend  where info like '%期如梦%' or info like '%中兴离异交友群%'or info like '%bbbbb%'or info like '%bbbbb%'or info like '%bbbbb%'or info like '%bbbbb%'")
                cursor.execute(sql)
                # 获取查询结果
                result = cursor.fetchall() #获取全部结果，取一条为 fetchone()

                head2=head.replace("%count",str(len(result)))
                f.write(head2)

                for tmp in result:
                    info=tmp['info']
                    imgString=""
                    if tmp['image'] != None :
                        for img in tmp['image'].split(","):
                            if img != '':
                                imgString=imgString+'<a href="%image"><image height="300px" style="" src="%image"></image></a>\n'.replace("%image",img)
                        trString=tr.replace("%info",tmp['info']).replace("%author",tmp['author']).replace("%date",str(tmp['date']))\
                            .replace("%image",imgString).replace("%id",str(tmp['id'])).replace("%tag",str(tmp['tag'] or ''))\
                            .replace("%class",str(tmp['tag'] or '').replace(","," "))

                    f.write(trString)
                #print(result)
            # 没有设置默认自动提交，需要主动提交，以保存所执行的语句
            connection.commit()
            f.write(tail)
            f.close()
        finally:
            connection.close();

sql="select * from log.friend where (tag = ''   or tag like '%推荐%' or tag like '%了解中%') and ( tag not like '%不合适%' and tag not like '%未通过%' and  tag not like '%推荐-1%'  ) or tag is NULL  order by date desc"
#sql="select * from log.friend where (  tag like '%年龄%')  order by date desc"
#sql= "select * from log.friend order by date desc limit 100 "
execte(sql)



********************************************
###F:\work2\10_python\python\sql.sql###
********************************************


UPDATE 	  friend set tag=concat(tag, '异地,')    where tag not like '%异地%' and ( info like '%深圳%' or info like '%西安%'or info like '%武汉%'or info like '%上海%'or info like '%bbbbb%'or info like '%bbbbb%');
delete from friend  where info like '%期如梦%' or info like '%中兴离异交友群%'or info like '%bbbbb%'or info like '%bbbbb%'or info like '%bbbbb%'or info like '%bbbbb%';


update friend set time = replace(time,'年','-');
update friend set time = replace(time,'月','-');
update friend set time = replace(time,'日','');
update friend set time = replace(time,'时',':');
update friend set time = replace(time,'分',':00');
update friend set  date = time;
update friend set tag=replace(tag,"推荐+1","") ;


********************************************
###F:\work2\10_python\python\translate.py###
********************************************

# coding=utf-8
import urllib
import urllib.request
import re
import http.cookiejar
from html.parser import HTMLParser
from bs4 import BeautifulSoup
import pymysql.cursors
import time
import datetime
import gzip


#https://www.jianshu.com/p/30b4fa6793f6



'''
将文档中的单词翻译，并加入到translate.html中
'''

header='''<!DOCTYPE html>
<html>
<head>
	<title></title>

<meta content="text/html; charset=utf-8" http-equiv="content-type" />
</head>
<body>

<table>  '''

footer ='''
</table>
</body>
</html>'''

def getHtml(word):

    cookieA='''ipv6=hit=1550456714996&t=4; SRCHD=AF=NOFORM; SRCHUID=V=2&GUID=0A1C54D5DE1A4CCE9F5B3C204AB3008A&dmnchg=1; _EDGE_V=1; MUID=0C8443A31AA1665721FA4EA21B8F6780; MUIDB=0C8443A31AA1665721FA4EA21B8F6780; _FP=hta=on; MSCC=1; btstkn=C6EtNzcUJuQ0VQzKNExR9Vro5S4keB%252Bn5Jm88OucVmRdoVpNsOfiEaUp1%252BDus09n; _TTSS_OUT=hist=["en"]; ULC=P=553B|7:2&H=553B|4:2&T=553B|7:2:1; ENSEARCH=BENVER=0; MSTC=ST=1; SRCHUSR=DOB=20190212&T=1550453084000; _tarLang=default=zh-CHS; SRCHHPGUSR=CW=1903&CH=935&DPR=1&UTC=480&WTS=63686049884; ipv6=hit=1550118302808&t=4; _SS=SID=2C56640832FE62AB3680690933D063F2&HV=1550453111&bIm=345578; SNRHOP=I=&TS=; _EDGE_S=mkt=zh-cn&F=1&SID=2C56640832FE62AB3680690933D063F2'''
    headers = {
"accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
"accept-encoding":"gzip, deflate, sdch",
"accept-language":"zh-CN,zh;q=0.8",
"cache-control":"no-cache",
"pragma":"no-cache",
"referer":"https://cn.bing.com/dict/",
"upgrade-insecure-requests":"1",
        "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36",
        "Cookie":cookieA,
    }
 #   opener = urllib.request.build_opener(proxy_handler,data,headers)
 #  urllib.request.install_opener(opener)
    url="https://cn.bing.com/dict/search?q="+word+"&go=%E6%90%9C%E7%B4%A2&qs=n&form=Z9LH5&sp=-1&pq=" + word + "&sc=7-4&sk=&cvid=113013BE5FDD4E8C93AFB77D17F4A9B2"
    print("visit url %s" % url)
    req = urllib.request.Request(url, None, headers)
    response = urllib.request.urlopen(req)


    html = gzip.decompress(  response.read()).decode("utf-8")

    #print(html)


    return html

config = {
        'host': '127.0.0.1',
        'port': 3306,
        'user': 'root',
        'password': '',
        'db': 'log',
        'charset': 'utf8mb4',
        'cursorclass': pymysql.cursors.DictCursor,
    }

#
connection = pymysql.connect(**config)

def parse(html):
    soup = BeautifulSoup(html,'html.parser')
    #print(soup.prettify())
    data1 = soup.find('div',{"class" : "hd_area"} )

    #data = soup.findAll('p' )
    #print(data1)
    data2 = ""
    if(soup.find('div',{"class" : "qdef"} ) is not None):
        data2 = soup.find('div',{"class" : "qdef"} ).find("ul")
    else:
        print ("translate fail")

    data3=soup.find("div",{"class":"hd_if"})

    data4=soup.find("div",{"id":"thesaurusesid"})

    #data = soup.findAll('p' )
    #print(data2)
    return "%s %s %s %s" % (data1 , data2,data3,data4)


def analyse(file,translatefile):
    # 读取文件
    fn = open(file,'r',-1,'utf8') # 打开文件
    string_data = "1"
    f=open(translatefile, 'a+', encoding='utf8')
    #f.write(header)
    string_data = fn.readline()
    while(string_data != ''  and string_data != None):
        string_data=re.sub('[\r\n\t]', '', string_data)
        print("translate word %s" % string_data)
        data=translate(string_data.strip())
        f.writelines("\n")
        f.write("<tr>")
        if len(data) > 30 :
            f.write(data)
        else:
            f.write(string_data.strip())
        f.write("</tr>")
        string_data = fn.readline()
        string_data=re.sub('[\r\n\t]', '', string_data)
        try:
                with connection.cursor() as cursor:
                    #执行sql语句，进行查询
                    sql="insert into log.word (word,info  ) VALUES ('%s','%s')" % (string_data,data)
                    #print(sql)
                    #cursor.execute(sql)
        finally:
                x=1;
        time.sleep(0)
    #f.write(footer)
    f.close()
    fn.close() # 关闭文件


def translate(word):
    html=getHtml(word)
    return parse(html)

analyse("translate",'translate5.html')
********************************************
###F:\work2\10_python\python\translateFromHtml.py###
********************************************

# coding=utf-8
import urllib
import urllib.request
import re
import http.cookiejar
from html.parser import HTMLParser
from bs4 import BeautifulSoup
import pymysql.cursors
import time
import datetime
import gzip

f=open("translate.html",'r',-1,"utf8")
fw=open("transtlathtml.html",'w',-1,"utf8")
html=f.read().lower()
soup = BeautifulSoup(html,'html.parser')
def parse(word):
    #print(soup.prettify())
    data =soup. find(name="strong",text=word)
    if data != None:
        data1=  data.parent.parent.parent.parent
        #print(data1)
        fw.write( "%s" % data1)
        fw.writelines("\n")
    else:
        print(word)

f = open("wordFromHtml")

line=f.readline()

while(line != ''):
    word=line.splitlines()[0].lower()
    parse(word)
    line =f.readline()
********************************************
###F:\work2\10_python\python\wordcut.py###
********************************************

import pylab
#%pylab inline
import jieba
from wordcloud import WordCloud
filename = "F:/work2/10_python/workspace/participle/words"
with open(filename,'r',-1,'utf8') as f:
 mytext = f.read()

mytext = " ".join(jieba.cut(mytext))
wordcloud = WordCloud(font_path="F:/work2/10_python/workspace/participle/simsun.ttf").generate(mytext)
import matplotlib.pyplot as plt



plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

********************************************
###F:\work2\10_python\python\__init__.py###
********************************************
